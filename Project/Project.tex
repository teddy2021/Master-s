\documentclass{article}

\author{Kody Manastyrski}
\title{CP 8316 Reinforcement Learning}
\date{April 2022}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{subfigure}

\begin{document}

\maketitle

\section{Question 1}
The max value of the system is 4.0, which is achieved through taking the path:
\[\{0, 2, 1, 2, 1\}\]

Suppose that there is a path which has a greater value than 4. 
For ease of notation we shall define approaching 4 from below as adding up from
$-\infty$, and approaching from above as subtracting from $\infty$. 
With the notation in mind, we see that there are two cases for a path to result
in a value greater than 4, but doesn't.
Either it approaches from below and adds some value,
$\Psi$, to the cumulative total to that point, or it approaches from above and
adds $-\Psi$ to the cumulative total.

In the first case, this implies either 
that $\Psi$ must either be a negative value with absolute value greater than 0.3,
or it must have a value greater than 1.
In either case, no such state exists. 

In the case where we approach 4 from above the value of $\Psi$ must take on the
value of either -0.1, -0.2, or -1.
This gives an above value of 4.1, 4.2, or 5.0. 
4.1 and 4.2 can only result when the cumulative total is 4, which we see results from 
a path that exhasts the horizon.
5.0 must result from a cumulative total of 4 added to 1 
(which itself results from the rewards of state 2 and 3 in concert), or 3 and 2 
(which is the result of the rewards of states 2 and 1 in concert).
In the former case, we already know that a cumulative total of 4 exhastst the  horizon.
For the latter case, there must be a cumulative sum of 3 which has at most 3 states included.
Since the first state is always 0, this removes 1 for the possibility space of states.
Thus there must be a state that either has value 3, which there isnt', or a pair of states
with value 3 together. 
Any of the states which add value (positively or negatively) have absolute value below 1, 
thus no 2 states sum together to 3. 
On the other hand transitioning from state 2 could result in 3 if the state transitioned
into has value -0.3. 
No such state exists, however, and thus approaching from above is impossible.

\section{Question 2}
\subsection{}
The benefit of having the result of $\hat{q}(s;w)\in \mathbb{R}^{|\mathbb{A}|}$
is that the scenario becomes a categorization problem for the value function. 
Similar to the mnist problem, the input will result in the maximal category (in
this case state) which is the objective of using the value function.

\subsection{}
The tests pass. No additional tests were implemented at initial time of writing.

\subsection{}

\subsection{}

\section{Question 3}
\subsection{}
The system occassionally reaches the max value, but mostly gets in the area. 
With repeated running of q3, the average reward scatters between 0.5 and 4.1.
The most recent running of the system is appended below.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{src/results/q3_linear/scores.png}
	\caption{Most recent result from running q3\_linear.py}
\end{figure}

\subsection{}
Running q4\_nature.py shows values more closely scattered, roughly around 2.05,
with more values above than below this value.
Training time takes longer, but still remains within a few minutes.
Like with the above example, the most recent running of q4\_nature.py has its 
score plot attatched below.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.75\textwidth]{src/results/q4_nature/scores.png}
	\caption{Most recent result from running q4\_nature.py}
\end{figure}

\section{Question 4}

\subsection{}
The average reward as time progresses remains at around -20. 
This suggests that the system managed to find a local minimum. 
With the methodology that is being used, further programming wouldn't likely 
result in much change. 
With simulated annealing, beam search, random restart, or something of the sort 
either to replace or agument stochastic gradient descent, it is more likely that
the system will approach more optimal results in the same timeframe, and better
performance given more time/iterations.


\subsection{}
At time of writing, the system has yet to finish. 
Unfortunately the project ran long in the tooth, and for training the only computers 
available are linux systems, which are not currently supported by cuda with the 
current kernel version (a quote from Linus Torvalds from the talk at Aalto
University), so my system is stuck training on a 5 year old i5 cpu. 
I don't anticipate it will finish by the deadline.


\section{Question 5}
For the same reason above, I cannot reasonably train an alternative model, and 
thus instead of programming it, have opted to focus my efforts on a project that 
is due in another class. 
I will mention that my idea was to run two q networks, augmented with simulated
annealing, and use the one with the minimal loss to update the target network.
The simulated annealing would be implemented similarly to the epsilon greedy
exploration strategy by adding an additional random chance to select the state 
which has the opposite consequence from the selected state. 
For example if an action, A, were to move the paddle up, then logically the opposite
action would be to move the paddle down. 
Thus with a chance determined by the 'temperature' of the system (updated each 
time a 'success' for the temperature chance is selected) the model would select
the opposite action. 
Depending on how this would play out, it may instead select a series of opposite actions
up to some threshold in order to sufficiently move the system away from a stable 
local minima. 


\end{document}
